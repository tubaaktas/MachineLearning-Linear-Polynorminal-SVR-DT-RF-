# -*- coding: utf-8 -*-
"""odev2BtkMachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iqt1tLVqnvXOUwM3W_PZEY56agqNnjFN
"""

#library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score #To look at the r2 scores of the algorithms we use
import statsmodels.api as sm #let's see if we need to eliminate columns

#import data
data = pd.read_csv('maaslar_yeni.csv')
#NOT: No id column is taken in machine learning because it causes memorization
'''
#x1-x2-x3
x = data.iloc[:,2:5]
y = data.iloc[:,5:]
X = x.values #independent variable
Y = y.values #dependent variable

print(data.corr())#corr matrix
'''
#x1 -> just one
x = data.iloc[:,2:3]
y = data.iloc[:,5:]
X= x.values #independent variable
Y= y.values #dependent variable

#linear regression 
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, Y)



#let's see if we need to eliminate columns using pi value
model=sm.OLS(lin_reg.predict(X), X) #Get the prediction results of X and compare with X
print(model.fit().summary()) #the results have a high value of x2 and x3. Let's just work on x1.

print("Linear R2 value:")
print(r2_score(Y, lin_reg.predict((X))) )

#polynominal regression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree = 4)
x_poly = poly_reg.fit_transform(X)
print(x_poly)
lin_reg2 = LinearRegression()
lin_reg2.fit(x_poly, y)

#predictions
#print(lin_reg2.predict(11))
#print(lin_reg2.predict(6.6))

#print(lin_reg2.predict(poly_reg.fit_transform(11)))
#print(lin_reg2.predict(poly_reg.fit_transform(6.6)))


print("Poly OLS ")
model2 = sm.OLS(lin_reg2.predict(poly_reg.fit_transform(X)),X)
print(model2.fit().summary())

print("Polynominal R2 value:")
print(r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))

#data scaling
from sklearn.preprocessing import StandardScaler

sc1= StandardScaler()
x_olcekli = sc1.fit_transform(X)
sc2 = StandardScaler()
y_olcekli = sc2.fit_transform(Y.reshape(-1,1))


from sklearn.svm import SVR
svr_reg = SVR(kernel="rbf")
svr_reg.fit(x_olcekli , y_olcekli)



print("SVR OLS ")
model3 = sm.OLS(svr_reg.predict(x_olcekli),x_olcekli)
print(model3.fit().summary())

print("SVR R2 value")
print(r2_score(y_olcekli, svr_reg.predict(x_olcekli)))


#Decision Tree Regression
from sklearn.tree import DecisionTreeRegressor
r_dt = DecisionTreeRegressor(random_state=0)
r_dt.fit(X, Y)

print("DT OLS ")
model4 = sm.OLS(r_dt.predict(X), X)
print(model4.fit().summary())

print("Decision Tree R2 Values")
print(r2_score(Y, r_dt.predict(X)) )


#Random Forest Regression
from sklearn.ensemble import RandomForestRegressor
rf_reg= RandomForestRegressor(n_estimators =10 , random_state=0)
rf_reg.fit(X,Y)


print("DT OLS ")
model5 = sm.OLS(rf_reg.predict(X), X)
print(model5.fit().summary())

print("Random Forest R2 Value")
print(r2_score(Y, rf_reg.predict(X)) )

#In short
print("--------------------------------------------")
print("Linear R2 Value")
print(r2_score(Y, lin_reg.predict(X)) )

#library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score #To look at the r2 scores of the algorithms we use
import statsmodels.api as sm #let's see if we need to eliminate columns

#import data
data = pd.read_csv('maaslar_yeni.csv')
#NOT: No id column is taken in machine learning because it causes memorization

#x1-x2-x3
x = data.iloc[:,2:5]
y = data.iloc[:,5:]
X = x.values #independent variable
Y = y.values #dependent variable

print(data.corr())#corr matrix
'''
#x1 -> just one
x = data.iloc[:,2:3]
y = data.iloc[:,5:]
X= x.values #independent variable
Y= y.values #dependent variable
'''
#linear regression 
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, Y)



#let's see if we need to eliminate columns using pi value
model=sm.OLS(lin_reg.predict(X), X) #Get the prediction results of X and compare with X
print(model.fit().summary()) #the results have a high value of x2 and x3. Let's just work on x1.

print("Linear R2 value:")
print(r2_score(Y, lin_reg.predict((X))) )

#polynominal regression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree = 4)
x_poly = poly_reg.fit_transform(X)
print(x_poly)
lin_reg2 = LinearRegression()
lin_reg2.fit(x_poly, y)


print("Poly OLS ")
model2 = sm.OLS(lin_reg2.predict(poly_reg.fit_transform(X)),X)
print(model2.fit().summary())

print("Polynominal R2 value:")
print(r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))

#data scaling
from sklearn.preprocessing import StandardScaler

sc1= StandardScaler()
x_olcekli = sc1.fit_transform(X)
sc2 = StandardScaler()
y_olcekli = sc2.fit_transform(Y.reshape(-1,1))


from sklearn.svm import SVR
svr_reg = SVR(kernel="rbf")
svr_reg.fit(x_olcekli , y_olcekli)



print("SVR OLS ")
model3 = sm.OLS(svr_reg.predict(x_olcekli),x_olcekli)
print(model3.fit().summary())

print("SVR R2 value")
print(r2_score(y_olcekli, svr_reg.predict(x_olcekli)))


#Decision Tree Regression
from sklearn.tree import DecisionTreeRegressor
r_dt = DecisionTreeRegressor(random_state=0)
r_dt.fit(X, Y)

print("DT OLS ")
model4 = sm.OLS(r_dt.predict(X), X)
print(model4.fit().summary())

print("Decision Tree R2 Values")
print(r2_score(Y, r_dt.predict(X)) )


#Random Forest Regression
from sklearn.ensemble import RandomForestRegressor
rf_reg= RandomForestRegressor(n_estimators =10 , random_state=0)
rf_reg.fit(X,Y)


print("DT OLS ")
model5 = sm.OLS(rf_reg.predict(X), X)
print(model5.fit().summary())

print("Random Forest R2 Value")
print(r2_score(Y, rf_reg.predict(X)) )

#In short
print("--------------------------------------------")
print("Linear R2 Value")
print(r2_score(Y, lin_reg.predict(X)) )